import requests
from lxml import html, etree
import concurrent.futures
import time
import traceback

# Ğ§Ğ¸ÑÑ‚ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ°
def clean_text(text_list):
    return ' '.join([text.strip() for text in text_list if text.strip()])

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞ¼Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚ĞµĞ¼Ñ‹ Ğ¸Ğ· URL
def extract_theme(url):
    parts = url.strip("/").split("/")
    theme = parts[3] if len(parts) > 3 else "ĞĞµÑ‚"
    
    # Ğ£Ğ±ĞµĞ´Ğ¸Ğ¼ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚ĞµĞ¼Ğ° Ğ¸Ğ¼ĞµĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¼ĞµĞ½ĞµĞµ 10 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²,
    subtheme = parts[4] if len(parts) > 4 and len(parts[4]) < 15 else "ĞĞµÑ‚"
    
    return theme, subtheme

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑĞ°Ğ¹Ñ‚Ğ°
def get_link(url):
    try:
        time.sleep(0.1)  # ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸
        response = requests.get(url, timeout=10)
        response.encoding = 'utf-8'

        if response.status_code != 200:
            print(f"ĞÑˆĞ¸Ğ±ĞºĞ° {response.status_code}: {url}")
            return None

        response.encoding = response.apparent_encoding  # ĞšĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ
        tree = html.fromstring(response.text)  # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ HTML Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸

        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸
        title_elements = tree.xpath('//*[@id="Inhalt"]//article//header//div//h2//span//text()')
        title_elements = [t.strip() for t in title_elements if t.strip()]
        short_title = title_elements[0] if title_elements else "ĞĞµÑ‚"
        main_title = ' '.join(title_elements[1:]) if len(title_elements) > 1 else short_title

        # ĞšÑ€Ğ°Ñ‚ĞºĞ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
        description_elements = tree.xpath('//*[@id="Inhalt"]//article//header//div//div//div[1]//text()')
        description = clean_text(description_elements) if description_elements else "ĞĞµÑ‚"

        # Ğ”Ğ°Ñ‚Ğ° Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
        date_elements = tree.xpath('//*[@id="Inhalt"]//article//header//div//div//div[3]//time//text()')
        date = clean_text(date_elements) if date_elements else "ĞĞµÑ‚"

        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ‚ĞµĞ¼Ñƒ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚ĞµĞ¼Ñƒ Ğ¸Ğ· URL
        theme, subtheme = extract_theme(url)

        return {
            "short_title": short_title,
            "main_title": main_title,
            "description": description,
            "date": date,
            "theme": theme,
            "subtheme": subtheme
        }
    except Exception as e:
        print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ {url}: {e}")
        print(traceback.format_exc())
        return None

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² XML
def save(file, new_data):
    if new_data is None:
        return

    try:
        with open(file, "ab") as f:  # Ğ—Ğ°Ğ¿Ğ¸ÑÑŒ
            article = etree.Element("article")
            etree.SubElement(article, "theme").text = new_data["theme"]
            etree.SubElement(article, "subtheme").text = new_data["subtheme"]
            etree.SubElement(article, "short_title").text = new_data["short_title"]
            etree.SubElement(article, "main_title").text = new_data["main_title"]
            etree.SubElement(article, "description").text = new_data["description"]
            etree.SubElement(article, "date").text = new_data["date"]
            f.write(etree.tostring(article, pretty_print=True, encoding="utf-8"))  
    except Exception as e:
        print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ² XML: {e}")
        print(traceback.format_exc())

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑÑÑ‹Ğ»Ğ¾Ğº
def process_links(file, urls, max_threads=10):
    with concurrent.futures.ThreadPoolExecutor(max_threads) as exe:
        futures = {exe.submit(get_link, url.strip()): url for url in urls}

        for count, future in enumerate(concurrent.futures.as_completed(futures), start=1):
            new_data = future.result()
            if new_data:
                save(file, new_data)
                print(f"[{count}] âœ… ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑÑÑ‹Ğ»ĞºĞ°: {futures[future]}")

# Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ
with open("links.txt", encoding="utf-8") as f:
    urls = [line.strip() for line in f if line.strip()]

process_links("data.xml", urls, max_threads=10)

print("ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©\n"
      "ğŸŸ©ğŸŸ©ğŸŸ© Ğ“ĞĞ¢ĞĞ’Ğ ğŸŸ©ğŸŸ©ğŸŸ©\n"
      "ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©")